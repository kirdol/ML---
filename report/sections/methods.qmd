---
title: "Methods"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

# Methods

This section provides a detailed explanation of the methods used in this study, aimed at ensuring reproducibility. The global strategy and each specific tool employed are described concisely, with relevant properties highlighted. Clear references are provided for further details where necessary.

## Supervised Learning

### Classification Tree

As our dataset is dealing with classes, we have decided to use a Classification tree to predict the make of the car.

Classification Trees are constituted of one root node and a hierarchical set of binary rules. Each node represent a decision on an attribute, each branch the outcome of the decision and each leaf a class label (The most important features are located at the top of the graph). Thus, the name is coming from the shape of the representation, that looks like a tree. Even if it can be also applied to regressions, we are using it in our case for classification tasks.

Key parameters and hyperparameters used in classification tree include:

**Parameters**

-   Data observations

-   Method (usually class, which specifies that the decision tree being constructed is for classification tasks)

**Hyperparameters**

-   **Minsplit** : minimum number of observations that must exist in a node for a split to be attempted

-   **minbucket** : controls the minimum size of terminal nodes

-   **complexity parameter cp** : controls the size of the decision tree. Also prevent overfitting.

-   **Maximum depth** : Set a maximum "length" of the tree, starting from the root node.

-   **Random state** : Set a specific number (usually 42) to control the randomness of the estimator.

-   **xval** : Helps in model validation by performing cross-validation

### Our two primary models

1.  Classification Tree without constraints

2.  Classification Tree with Pruning (Setting a limit for the max depth)

In both cases, we are using training and test accuracy in order to know when there is overfitting.

### Classification Tree without constraints

The classification tree model serves as the baseline. It consists of the following components:

-   A Root Node: the highest node in a tree. It represents the entire dataset, before being splitting according to the best features that are going to separate the observations according to specific criterions.

-   Internal Nodes: the remaining nodes that are not the root or a leaf node. Each internal node splits the data into two or more subsets based on a feature and a threshold value.

-   Branches and Edges: connections between the nodes. Represent the outcome of a decision performed at each node.

-   Leaf node: final decision/classification. Each node correspond to a class label.

In our case, the Decision Tree Classifier creates the decision tree. It initialize the process of recursively splitting the dataset into subsets based on the value of the features. It also implicitly defines the loss function, with the two following most common criteria : Gini Impurity (measuring how mixed are the classes in a node, also selecting the best features) and Entropy (different from the Logistic Regression, measure of chaos, indicating the uncertainty reduced by the split). Even by using the stopping rules seen in class (minsplit, cp, minbucket), the tree can sometimes grow too long. Therefore, we need to prune the tree.

### Classification Tree with Pruning

Now let's talk about the classification tree using pruning.

We have the same components as in the base method. Nevertheless, some part of the process is different. In this case we are using a complexity parameter to control the size of the tree. This threshold prevents overfitting.

One popular method for pruning is the 1-SE rule, which involves the following steps: first build a fully grown tree using standard stopping rules; then calculate the error for all possible sub-trees. The best tree with the lowest error is identified and its standard error (SE) is calculated. Trees with an error within one SE of the error of the best tree are considered equivalent, and among these the shortest tree is selected.

In our case, we have decided to tune the max_depth hyperparameter to limit the maximum depth of the decision tree.

#### Software and Tools

-   rpart : Used for building classification and regression trees

-   pandas : Library used for data manipulation and analysis.

-   Scikit-learn: Used for data preprocessing, splitting the dataset, and calculating class weights.

-   matplotlib : Used for creating static, animated, and interactive visualizations

-   seaborn : Provides a high-level interface for drawing attractive and informative statistical graphics

#### Sequence of Analysis

After loading and importing our necessary functions R and Python functions, we have prepared the data by first defining the make as the target variable, then encoding categorical variables using **Label Encoding** to convert them into numerical values. This step is essential as classification tree needs numerical value to work.

We then splited the dataset into training (80%) and testing (20%). This step is essential for preventing overfitting, try to select the best model configuration by training the model, then assess the final model performance on our "unseen data" of the testing set. This helps to build a robust model that generalizes well to new data.

Once the separation done, we finally have trained our decision tree, using the parameters and hyperparameters talked earlier. Then, thanks to the training model, we have been able to use the model on the test set and then to compute the accuracies of both. This step is essential for controlling the performance of the model, also controlling for overfitting.

After verification, we have found overfitting after constructing the base Classification Tree model. We have then decided to prune our tree, by setting different max_depth value in order to see which tradeoff between accuracy and performance (fighting overfitting) was the best.

### Neural Network

The use of a neural network was used for a classification task. As already discussed, we want to predict the make of a car utilizing the other features.

A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected layers of nodes (neurons) where each connection has an associated weight. The network learns to map input data to the desired output by adjusting these weights through a process called training. The model can be tuned using hyperparameters set before the learning process begins that governs the overall behavior and performance of the machine learning model.

Key parameters and hyperparameters used in neural networks include:

-   **Learning Rate:** Controls how much the model’s weights are updated with respect to the gradient of the loss function.

-   **Number of Epochs:** The number of complete passes through the training dataset.

-   **Batch Size:** The number of training examples used in one iteration to update the model’s weights.

-   **Number of Layers:** Determines the depth of the network, including input, hidden, and output layers.

-   **Number of Neurons per Layer:** The size of each layer, influencing the capacity of the model to learn from data.

-   **Activation Functions:** Non-linear functions applied to the output of each neuron, such as ReLU, sigmoid, or softmax.

-   **Optimizer:** The algorithm used to update the weights, such as Adam, SGD, or RMSprop.

-   **Loss Function:** Measures the difference between the predicted and actual output, guiding the optimizer, e.g., categorical cross-entropy for classification tasks.

-   **Dropout Rate:** The fraction of neurons to drop during training to prevent overfitting.

-   **Class Weights:** Used to handle imbalanced datasets by giving more importance to underrepresented classes.

##### Preprocessing the data

An important step before training our model has been to separate numerical and categorical columns in our data preprocessing because they require different types of handling to prepare them for machine learning algorithms. The numerical columns need to be scaled by adjusting them so they have a mean of zero and a standard deviation of one, which helps the machine learning algorithm perform better. While the categorical columns need to be one-hot encoded which creates a binary column a format that the machine learning model can understand.

#### Our three primary models

1.  A Simple Neural Network Model

2.  A Class-Weighted Neural Network Model

3.  And a Dropout-Enhanced Neural Network Model (also using class-weight)

All three models used the cleaned data set prepared in the data cleaning section. We chose the version with more than 10 observation for each car brands to avoid over imbalanced classes.

Each model’s architecture, training process, and evaluation metrics are described in the following sections.

##### Simple Neural Network Model

The simple neural network model serves as the baseline. It consists of the following components:

-   Input Layer: Matches the number of features in the dataset.

-   Hidden Layers: Two dense layers with ReLU activation functions.

-   Output Layer: A dense layer with a softmax activation function for classification. The softmax activation function output a probability for each features input to belong to a specific class (brand).

The model is trained using the Adam optimizer, categorical cross-entropy loss function, and accuracy as the primary metric.

##### Class-Weighted Neural Network Model

To address class imbalances, a class-weighted approach has then been applied. This involves assigning higher weights to the minority classes during training. The architecture of the network remains the same as the simple model. The following steps were taken:

-   Class Weights Calculation: Inverse proportionality to class frequencies.

-   Model Training: Incorporating class weights into the loss function.

The use of class weights helps in penalizing misclassifications of the minority classes more heavily, thereby improving the model’s performance on imbalanced data.

##### Dropout-Enhanced Neural Network Model

The third model incorporates dropout layers to mitigate overfitting while keeping the modification applied in our second model. Dropout layers randomly set a fraction of neurons to zero at each update during training, which helps prevent the network from becoming overly reliant on specific neurons. The deactivation of some neurons only happend during the training phases. When the model is used against the validation and test set, all neurons are active. This has the consequence of seeing higher accuracy on the validation set than the training set.

-   Model Architecture: Similar to the simple model with additional dropout layers after each dense hidden layer.

-   Dropout Rate: Will be tuned during the modeling.

The dropout-enhanced model helps improve generalization by reducing the risk of overfitting to the training data.

#### Software and Tools

The following software and tools were used to implement and evaluate the models (non-exhaustive list):

-   Python: The programming language used for all implementations.

-   TensorFlow and Keras: Libraries used for building and training neural network models.

-   Scikit-learn: Used for data preprocessing, splitting the dataset, and calculating class weights.

-   Pandas and NumPy: Libraries used for data manipulation and numerical computations.

#### Mertics

In our machine learning analysis, we used several key metrics to evaluate our model. The main metrics are the training, validation and test accuracies. The **validation accuracy** shows how well the model performs on the validation set, helping us tune the model during training. **Training accuracy** indicates the model’s performance on the training data, which helps identify overfitting. The difference between the training and validation accuracy is useful to detect the presence of overfitting in our models. The **test accuracy** measures the model’s performance on new, unseen data, providing an unbiased evaluation.

We also used the **sensitivity** to measure the model’s ability to correctly identify positive casesand the **specificity** that measures the ability to correctly identify negative cases. Even though, when we started to create our model, we already knew that we had unbalanced classes, these metrics confirmed this with numbers. The last metric that has been used is the **Cohen’s kappa** that evaluates the agreement between predicted and true labels, accounting for chance, making it more reliable than simple accuracy, especially with imbalanced datasets. These metrics together give a comprehensive view of the model’s performance.

#### Sequence of Analysis

We first encode the categorical variables and normalized the numercial features. We then trained the models and evaluated each one. By evaluating and investigating the model at each step we managed to deal with the different challenges that we faced. We also managed to tune the dropout rate to insure to keep a good model performance while highly decreasing the case of overfitting we were having.

This sequence ensures a systematic approach to model development and evaluation, providing a clear understanding of each step involved.

Note that this section was written and based on the course website ([ML_BA](https://do-unil.github.io/mlba/)). The parts on the dropout layers is based on the following links: [towardsdatascience](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9) and [python-course](https://python-course.eu/machine-learning/dropout-neural-networks-in-python.php#:~:text=The%20dropout%20approach%20means%20that,learn%20set%20with%20this%20network.)

## Unsupervised Learning

### Principal Component Analysis

-   combination of features
-   scaling
-   projection & variance proportion
-   circle of correlation
-   cos2
-   biplot
-   screeplot

To inspect the data, find/explain clusters, find dependence between the features. PCA can be used for EDA.

To diminish the number of features when there are too many: dimension reduction only keep few first PC.

PCA can only be performed on numerical features. When categorical features are also included in the analysis,

for ordinal data, quick and dirty solution: modalities can be mapped to numbers ( ) respecting their order,

for nominal data: there is no correct solution; especially replacing by numbers is incorrect.

With only categorical data, (Multiple) Correspondence Analysis is a solution. And for mixed data type (categorical and numerical), Factor Analysis of Mixed Data (FAMD) is a solution. However, they are not adapted to large data set.

### Clustering

-   Partitioning Method
-   Numerical --\> as we converted into numerical & factors into numerical, use K-Means

For clusters:

The initial clustering is random: assign each instance to one cluster at random.

Compute the centers of the clusters.

Each instance is then re-assigned to the cluster with the closest center.

Step 2. and 3. are repeated until a convergence criterion is satisfied.

-K-means

-   Number of cluster
-   TWCS
-   Elbow
-   Silhouette

As we have seen several unsupervised learning tools in class, we have looked at our dataset and decided to start with a Principal Component Analysis, as we have some categorical variables as well as numerical ones. This technique allows to combine features in fewer dimensions according to their similarities. We then proceeded with a clustering method and combined both of them into a vizualisation in order to have a clearer result.

To begin the method, it is crucial to standardize the data, meaning to transform the categorical variables into factors, which in turn is transformed into numerical. Then, to have a small idea of the link between the features before attacking the PCA, we will a correlation heatmap, showing which variable really seem to be somewhat correlated, whether positively and negatively. We will start with the PCA right after and then a screeplot, all of this in order to see the weight of the dimensions as well as the observations in the PCA graph.

Depending on the results of the PCA, we might consider proceeding with a clustering to have a clearer overview of the similarities in the observations and divide them into clusters

To achieve this, the main aspect to consider is to take the results from the PCA. So, to begin with, we will take the PCA coordinates and then proceed with a K-means method. We will then perform an elbow method to check for the optimal number of clusters. Depending on the results, we might consider doing a silhouette plot to check for heterogeneity in the clusters. Finally, as we are doing the PCA right before the clustering, we plan to create a 3D biplot where both the features and the clustered observations can be seen in order to interpret the final results for the unsupervised learning part.

The final aim is to see the link between the features as well as the similarities of the observations.
