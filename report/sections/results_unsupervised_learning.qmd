---
title: "Unsupervised Learning"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, message = F, warning = F}
source(here::here("scripts","setup.R"))
library(data.table)
data_cleaned <- fread(here::here("data", "data_cleaned.csv"))
```

In order to see the link between the features, we can use a dimension reduction technique such as the Principal Component Analysis, aiming to link the features according to their similarities accross instances and combine features in fewer dimensions.

# Principal Component Analysis

## Data Standardization

```{r, message = F, warning = F}
data_prepared <- data_cleaned %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  scale()  # Standardizes numeric data including converted factors
```

## Heatmap

```{r, message = F, warning = F}
cor_matrix <- cor(data_prepared)  # Calculate correlation matrix

# Melt the correlation matrix for ggplot2
melted_cor_matrix <- melt(cor_matrix)

# Heatmap with all correlation coefficients displayed
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +  # Add white lines to distinguish the tiles
  geom_text(aes(label = sprintf("%.2f", value)), color = "black", size = 3.5) +  # Always display labels
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "blue", midpoint = 0, limit = c(-1,1),
                       name = "Spearman\nCorrelation") +  # Use gradient2 for a diverging color scheme
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5),  # Center the title
        plot.title.position = "plot") +
  labs(x = 'Variables', y = 'Variables', 
       title = 'Correlations Heatmap of Variables')  # Adjust the title and labels as needed
```

We used this heatmap to check the correlation between the variables. As we can see, some variables seem to be strongly correlated, but most of them don't seem to be too strongly correlated, whether positively or negatively. Let's now look into the link between the features using a biplot, which combines the observations as well as the features.

## Biplot

```{r, message = F, warning = F}
pca_results <- PCA(data_prepared, graph = FALSE)
summary(pca_results)

fviz_pca_biplot(pca_results,
                geom.ind = "point",  
                geom.var = c("arrow", "text"), 
                col.ind = "cos2",  
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  
                repel = TRUE 
)
```

The biplot shows several information. First, the two dimensions epxlain almost 50% of the total variance of the data. Then, each point represents an observation and its color represent the quality of the representation of the variables. Looking at the cos2 gradient, the redder the dot, the better the quality of representation. Then, the arrows (or vectors) represent the features. The vectors pointing in similar directions represent some positive correlations, whereas the ones going opposite represent negative correlations. Orthogonal variables are not correlated.

Taking all of these into account, we can interpret this graph in the following way: we notice that the variables linking the mpg and the range for a fuel type (e.g. fuel type 1) go in the same direction and all seem to be positively correlated, and they are uncorrelated to the same characteristics of the other fuel type (e.g. fuel type 2). Also, the mpg and range seem to be negatively correlated to their own fuel type. Moreover, fuel type 1 is uncorrelated to fuel type 2, which makes sense.

## Screeplot

```{r, message = F, warning = F}
# Use PCA results to generate the screeplot
fviz_eig(pca_results, 
         addlabels = TRUE,  
         ylim = c(0, 100),  
         barfill = "lightblue",  
         barcolor = "black",  
         main = "Scree Plot of PCA") 
```

Taking the screeplot into account, 7 dimensions are needed to reach at least 80% of the variance, meaning the features might be relatively independent. It is already shown in the biplot above, as most arrows in the middle seem to be shorter and the cos2 are low, meaning that the features might be more linked to other dimensions than the first 2 dimensions. As a result of the biplot, we will try to provide a clustering in order to potentially see the different clusters of features similarities.

# Clustering

## Clusters

```{r, warning = F, message = F}
# Extract PCA coordinates for clustering
pca_coords <- pca_results$ind$coord

# Using the Elbow Method to determine the number of clusters
set.seed(123)  
wss <- sapply(1:15, function(k) {
  kmeans(pca_coords, centers = k, nstart = 25)$tot.withinss
})

# Elbow Plot
plot(1:15, wss, type = "b", pch = 20, frame = FALSE, 
     xlab = "Number of clusters K", ylab = "Total within-clusters sum of squares")

# Optimal number of cluster according to Elbow
k <- 4

# K-means
set.seed(123) 
km_result <- kmeans(pca_coords, centers = k, nstart = 25)

# Add cluster results and observation names to the PCA coordinates data frame
pca_coords_df <- as.data.frame(pca_coords)
pca_coords_df$cluster <- factor(km_result$cluster)
pca_coords_df$name <- rownames(data_cleaned) 

# Visualize the clusters using PCA and label points with observation names
ggplot(pca_coords_df, aes(x = Dim.1, y = Dim.2, color = cluster, label = name)) +
  geom_point() +
  geom_text(check_overlap = TRUE, vjust = 1.5, size = 3) +  
  labs(title = "Cluster Plot", x = "Dim1 (20%)", y = "Dim2 (20%)") +
  scale_color_manual(values = c("lightblue", "lightpink", "lightgreen", "lightgrey")) +  
  theme_minimal() +
  theme(legend.position = "right")  
```

## Biplot & Clusters

```{r, warning = F, message = F}
# Calculate Cluster Centers
cluster_centers <- aggregate(pca_coords_df[, 1:2], by = list(cluster = pca_coords_df$cluster), FUN = mean)

# Biplot with clusters
fviz_pca_biplot(pca_results,
                geom.ind = "point", 
                geom.var = c("arrow", "text"),  
                col.ind = factor(km_result$cluster), 
                palette = c("lightblue", "lightpink", "lightgreen", "lightgrey"), 
                addEllipses = TRUE, 
                ellipse.level = 0.95, 
                repel = TRUE,  
                legend.title = "Cluster") +
  geom_text(data = cluster_centers, aes(x = Dim.1, y = Dim.2, label = paste("Cluster", cluster)), 
            color = "black", size = 5, vjust = -1) +
  labs(title = "PCA - Biplot",
       x = paste("Dim1 (", round(pca_results$eig[1,2], 1), "%)", sep = ""),
       y = paste("Dim2 (", round(pca_results$eig[2,2], 1), "%)", sep = ""))
```

Using a clustering methods allow us to see the number of clusters that share similarities. Here, the elbow indicates that the optimal number of clusters should be 4. When comparing it with the biplot (last graph), we clearly notice that cluster 1 refers to the characteristics (range and mpg) for fuel 1 and same for fuel 4. Cluster 2 and 3 are packed towards the center and it is harder to see which features are similar, which is explainable with the fact that we would need around 6 dimensions to explain the total variance. We can suppose that cluster 2 refers to the class and the vehicle transmission, whereas cluster 3 focuses on engines and cylinders.