---
title: "Unsupervised Learning"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, message = F, warning = F}
source(here::here("scripts","setup.R"))
library(data.table)
data_cleaned <- fread(here::here("data", "data_cleaned.csv"))
```

In order to see the link between the features, we can use a dimension reduction technique such as the Principal Component Analysis, aiming to link the features according to their similarities across instances and combine features in fewer dimensions.

# Principal Component Analysis

## Data Standardization

```{r, message = F, warning = F}
data_prepared <- data_cleaned %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  scale()  # Standardizes numeric data including converted factors
```

We begin by standardizing our data, meaning trasnforming our character variables into factors and then the factors into numeric.

## Heatmap

```{r, message = F, warning = F}
cor_matrix <- cor(data_prepared)  # Calculate correlation matrix

# Melt Correlation Matrix
melted_cor_matrix <- melt(cor_matrix)

# Heatmap
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") + 
  geom_text(aes(label = sprintf("%.2f", value)), color = "black", size = 3.5) +  
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "blue", midpoint = 0, limit = c(-1,1),
                       name = "Spearman\nCorrelation") +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5),  
        plot.title.position = "plot") +
  labs(x = 'Variables', y = 'Variables', 
       title = 'Correlations Heatmap of Features')  
```

We used this heatmap to check the correlation between the variables. As we can see, some variables seem to be strongly correlated, but most of them don't seem to be too strongly correlated, whether positively or negatively. Let's now look into the link between the features using a biplot, which combines the observations as well as the features.

## Biplot

```{r, message = F, warning = F}
pca_results <- PCA(data_prepared, graph = FALSE)
summary(pca_results)

# Biplot Graph
fviz_pca_biplot(pca_results,
                geom.ind = "point",  
                geom.var = c("arrow", "text"), 
                col.ind = "cos2",  
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  
                repel = TRUE 
)
```

The biplot shows several information. First, the two dimensions epxlain almost 50% of the total variance of the data. Then, each point represents an observation and its color represent the quality of the representation of the variables. Looking at the cos2 gradient, the redder the dot, the better the quality of representation. Then, the arrows (or vectors) represent the features. The vectors pointing in similar directions represent some positive correlations, whereas the ones going opposite represent negative correlations. Orthogonal variables are not correlated.

Taking all of these into account, we can interpret this graph in the following way: we notice that the variables linking the mpg and the range for a fuel type (e.g. fuel type 1) go in the same direction and all seem to be positively correlated, and they are uncorrelated to the same characteristics of the other fuel type (e.g. fuel type 2). Also, the mpg and range seem to be negatively correlated to their own fuel type. Moreover, fuel type 1 is uncorrelated to fuel type 2, which makes sense.

## Screeplot

```{r, message = F, warning = F}
# Use PCA results to generate the screeplot
fviz_eig(pca_results, 
         addlabels = TRUE,  
         ylim = c(0, 100),  
         barfill = "lightblue",  
         barcolor = "black",  
         main = "Scree Plot of PCA") 
```

Taking the screeplot into account, 7 dimensions are needed to reach at least 80% of the variance, meaning the features might be relatively independent. It is already shown in the biplot above, as most arrows in the middle seem to be shorter and the cos2 are low, meaning that the features might be more linked to other dimensions than the first 2 dimensions. As a result of the biplot, we will try to provide a clustering in order to potentially see the different clusters of features similarities.

# Clustering

## Clusters

```{r, message = F, warning = F}
# Get PCA coordinates
pca_coords_df <- data.frame(pca_results$ind$coord)

# Perform K-means clustering
set.seed(123) # for reproducibility

# Use Elbow Method
wcss <- vector()
for (i in 1:10) {
  km <- kmeans(pca_coords_df[, 1:3], centers = i)
  wcss[i] <- sum(km$withinss)
}

# Elbow Plot
elbow_data <- data.frame(Clusters = 1:10, WCSS = wcss)
elbow_plot <- ggplot(elbow_data, aes(x = Clusters, y = WCSS)) +
  geom_point() +
  geom_line() +
  ggtitle("Elbow Method for Determining Optimal Number of Clusters") +
  xlab("Number of Clusters") +
  ylab("Within-Cluster Sum of Squares (WCSS)")

# Print the elbow plot
elbow_plot

# Looking at Elbow, Optimal nb of clusters = 5
optimal_clusters <- 5
km_result <- kmeans(pca_coords_df[, 1:3], centers = optimal_clusters)
```

Using the Elbow method for clustering, we can see that the optimal number of cluster is 5, which makes sense regarding the link of our features in our PCA. To dive deeper and better see the clusters, we have decided to provide a 3D biplot, which includes the PCA and the clustering.

## 3D Biplot

```{r, message = F, warning = F}
# Add cluster assignments to PCA coordinates
pca_coords_df$cluster <- km_result$cluster

# Calculate Cluster Centers
cluster_centers <- aggregate(pca_coords_df[, 1:3], by = list(cluster = pca_coords_df$cluster), FUN = mean)

# Get loadings for variables
loadings <- data.frame(
  variable = rownames(pca_results$var$coord),
  pca_results$var$coord[, 1:3]
)

# Create the 3D scatter plot with clusters and loadings
fig <- plot_ly() %>%
  add_trace(
    data = pca_coords_df,
    x = ~Dim.1,
    y = ~Dim.2,
    z = ~Dim.3,
    color = ~factor(km_result$cluster),
    colors = c("lightblue", "lightpink", "lightgreen", "lightgrey", "beige"),
    type = 'scatter3d',
    mode = 'markers',
    marker = list(size = 5)
  ) %>%
  add_trace(
    data = cluster_centers,
    x = ~Dim.1,
    y = ~Dim.2,
    z = ~Dim.3,
    text = ~paste("Cluster", cluster),
    type = 'scatter3d',
    mode = 'text',
    textposition = 'top center',
    textfont = list(color = 'black', size = 12)
  )

# Scale factor for loadings arrows
scale.loads <- 10.0

# Add loadings as arrows
for (k in 1:nrow(loadings)) {
  fig <- fig %>%
    add_trace(
      x = c(0, loadings$Dim.1[k]) * scale.loads,
      y = c(0, loadings$Dim.2[k]) * scale.loads,
      z = c(0, loadings$Dim.3[k]) * scale.loads,
      type = 'scatter3d',
      mode = 'lines+markers',
      line = list(width = 4, color = 'blue'),
      marker = list(size = 2, color = 'blue'),
      showlegend = FALSE
    ) %>%
    add_trace(
      x = loadings$Dim.1[k] * scale.loads,
      y = loadings$Dim.2[k] * scale.loads,
      z = loadings$Dim.3[k] * scale.loads,
      text = loadings$variable[k],
      type = 'scatter3d',
      mode = 'text',
      textposition = 'top center',
      textfont = list(color = 'blue', size = 10),
      showlegend = FALSE
    )
}

# Layout
fig <- fig %>%
  layout(
    title = "PCA - 3D Biplot with Features",
    scene = list(
      xaxis = list(title = paste("Dim1 (", round(pca_results$eig[1, 2], 1), "%)", sep = "")),
      yaxis = list(title = paste("Dim2 (", round(pca_results$eig[2, 2], 1), "%)", sep = "")),
      zaxis = list(title = paste("Dim3 (", round(pca_results$eig[3, 2], 1), "%)", sep = ""))
    )
  )

# Display the plot
fig
```

To combine both PCA and clustering, we first started with assigning the PCA coordinates to the clusters, then calculate the centers. We wanted to have the biplot such that we can rotate the plot and see both the observations divided into 5 clusters as well as the features arrows.

Looking at the plot, we can observe interesting features similarities. Let's have a look through each cluster. To begin with, cluster 1 seems to link the features regarding the range and mpg of fuel type 2. Then, cluster 2 seem to focus on the features regarding fuel type 1, as well as the drive and the transmission. Next is cluster 3 which seem to mainly link the range of fuel type 1, but also the charge time and moderatlely the mps. Then, cluster 4 englobes both engine displacement and cylinders as well as the vehicle class and the drive. Cluster 5 mainly considers make being moderately linked to mpg of fuel type 1, and somewhat to both fuel type 1 and fuel type 2. Model year seems to also be somewhat linked. 

In addition to that, looking at the dimensions, we can generalize by saying that dimension 1 links the features regarding fuel type 1, dimension 2 moderately the features regarding fuel type 2 and dimension 3 connects the technical car features (engine, class, make).