---
format:
  html:
    code-fold: true
---

# Classificatin Tree

In this section we are going to perform a classification tree analysis on the dataset. We will first load the necessary packages and the dataset. We then prepare the data by encoding categorical variables and splitting it into training and testing sets. We then tried to pruned the tree with different max_depth values to find the optimal tree depth that balances between training and test accuracy.

```{r Loading R packages CT, echo= FALSE, message = FALSE, output = FALSE}
source(here::here("scripts","setup.R"))
```

```{python Loading python Librairies CT, echo = FALSE, message = FALSE, waring = FALSE, output = FALSE}
import subprocess
subprocess.run(['pip3', 'install', "pyprojroot"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
from pyprojroot.here import here
open(here("scripts/setup.py"))
```


```{python Importing necessayry functions and the data, echo = FALSE}
# !pip3 install pyprojroot ----- TO INSTALL IF MISSING
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Load the dataset
file_path = here("data/data_cleaned.csv")
data = pd.read_csv(file_path)
```

We first loaded the dataset and identified make as the target variable. We also encoded categorical variables using Label Encoding to convert them into numerical values.

```{python Preparing the data, echo = FALSE}
# Preprocess the data
# Assuming 'make' is the target variable and all other columns are features
target = 'make'
features = data.columns.drop(target)

# Encode categorical variables
label_encoders = {}
for column in data.columns:
    if data[column].dtype == 'object':
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le
```

We then splited the dataset into training (80%) and testing (20%) sets to be able to evaluate the model's performance on unseen data after the training to check wheter the model is overfitting or not. We will see that it does.

```{python Splitting the data, echo = FALSE}
# Split the data into training and testing sets
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

Trained a Decision Tree classifier on the training data without any constraints.
The "None" case below represent the case without the pruning of the tree. As we can see, we observed overfitting, with high accuracy on training data and slightly lower accuracy on test data. Therefore, we decided to prune the tree as it as the advantage so simplify models and therefore limit overfitting. We chose to prune the tree by trying a few max_depth parameter values to control the tree's growth (none, 5, 10, 15, 20, 25, 30). We want here to find the optimal tree depth that balances between training and test accuracy.

```{python, echo = FALSE}
# Define a function to train and evaluate the Decision Tree with different max_depth values
def train_and_evaluate_tree(max_depth):
    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
    clf.fit(X_train, y_train)
    y_pred_train = clf.predict(X_train)
    y_pred_test = clf.predict(X_test)
    accuracy_train = accuracy_score(y_train, y_pred_train)
    accuracy_test = accuracy_score(y_test, y_pred_test)
    return accuracy_train, accuracy_test

# Evaluate the model with different max_depth values
max_depth_values = [5, 10, 15, 20, 25, 30, None]
results = {}

for depth in max_depth_values:
    accuracy_train, accuracy_test = train_and_evaluate_tree(depth)
    results[depth] = (accuracy_train, accuracy_test)

# Print the results
print("max_depth\tTraining Accuracy\tTest Accuracy")
for depth, (train_acc, test_acc) in results.items():
    print(f"{depth}\t\t{train_acc:.4f}\t\t{test_acc:.4f}")
```

The model's accuracy improved as the tree's depth increased up to a point, with a max_depth of 25 or 30 providing the best test accuracy up to 70%. We see that reducing the max_depth to 10 or 15 improves the balance between, therefore reduce drastically the case of overfitting but this is at the expense of the accuracy of our model on new data. But we can see that pruning the tree with a max depth of 25 allows us to increase our accuracy from 69.84% to 70% therefore increasing the accuracy of our model and at the same time, it reduce the gap between the test set and the trainig set. In our case,  pruning the Decision Tree helps in improving its generalization performance by preventing it from becoming too complex and reduce overfitting the training data.
