---
format:
  html:
    code-fold: true
---

```{r Loading R packages NN, echo= FALSE, message = FALSE, output = FALSE}
source(here::here("scripts", "setup.R"))
```

```{python Loading python Librairies NN, echo = FALSE, message = FALSE, waring = FALSE, output = FALSE}
import subprocess
subprocess.run(['pip3', 'install', "pyprojroot"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
from pyprojroot.here import here
open(here("scripts/setup.py"))
import pandas as pd
import numpy as np
from pyprojroot.here import here
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.utils import to_categorical
from keras.optimizers import Adam
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, cohen_kappa_score
from sklearn.model_selection import KFold

num_epochs = 150
```

# Neural Network

In this section, we will build a neural network model to predict the make of a car based on the features at our disposal. We will preprocess the data, split it into training and testing sets, define the neural network architecture, compile the model, train it and evaluate its performance.

## Preprocessing and splitting the data

The dataset contains different types of data. Some columns are numerical (like "city_mpg_fuel_type_1" or "charge_time_240v"), and some are categorical ("vehicle_class" or "fuel_type"). We identify and separate these two types of columns and preprocessed them.


```{python, echo = FALSE}
# Load the data
data = pd.read_csv(here("data/data_cleaned_reduced.csv"))

# Identify categorical and numerical columns
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Remove the target column 'make' from the features list
if 'make' in categorical_cols:
    categorical_cols.remove('make')
if 'make' in numerical_cols:
    numerical_cols.remove('make')

# Define the preprocessing steps for numerical and categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(sparse_output=False), categorical_cols)])

# Split data into features and target
X = data.drop('make', axis=1)
y = data['make']

# Apply preprocessing
X_preprocessed = preprocessor.fit_transform(X)

# Encode the target variable
y_encoded = pd.get_dummies(y).values
```

The data is split into two parts: training and testing. The training set is used to train the model, and the testing set is used to evaluate its performance. This split ensures that we can test how well the model generalizes to new, unseen data.

```{python, echo = FALSE}
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=123)

# Calculate class weights
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}
```

### Building the neural network models and training them

#### Base Neural Network
We chose to use a neural network. This neural network consists of layers of neurons, where each layer applies transformations to the data. The first layer takes the input features. Then some Hidden layers help the model learn complex patterns. In the end, the output layer predicts the probability of each car manufacturer. The first layes, the input layer, takes the input features. The second layers is set to 128 neurons, the third to 64 neurons and the last layer, the output layer, has as many neurons as there are car manufacturers. The activation function used in the hidden layers is the Rectified Linear Unit (ReLU), and the output layer uses the Softmax activation function. The model is compiled with the Adam optimizer and the categorical crossentropy loss function.

```{python}
#| code-fold: false

# Define the neural network model
model_no_dropout = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(y_train.shape[1], activation='softmax')])
```

We used activation functions in the hidden layers to introduce non-linearity into the model. The ReLU activation function is used in the hidden layers because it is computationally efficient and helps the model learn complex patterns in the data. The Softmax activation function is used in the output layer because it converts the model's raw output into probabilities that sum to one. This allows us to interpret the model's output as the probability of each car manufacturer.

####### We used the following hyperparameters (non-exhaustive list):

* **epochs:** 150 (Corresponds to the number of times the model sees the entire dataset during training.)
* **batch_size:** 32 (Corresponds to the number of samples that the model processes before updating the weights.)
* **validation_split:** 0.2 (Corresponds to the fraction of the training data to be used as validation data.)

The model is trained for 5 epochs with a batch size of 32. The validation split is set to 0.2, which means that 20% of the training data is used for validation.

```{python, echo = FALSE, output = FALSE}
# Compile the model
model_no_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model with class weights
history_no_dropout = model_no_dropout.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, verbose=0)

# Evaluate the model
loss_no_dropout, accuracy_no_dropout = model_no_dropout.evaluate(X_test, y_test)
print(f'Test accuracy without Dropout: {accuracy_no_dropout}')

# Make predictions
predictions_no_dropout = np.argmax(model_no_dropout.predict(X_test), axis=1)

# Print predictions
print(predictions_no_dropout)
```

Overall, the model perform well but we have't dealt with the issue of unbalanced classes yet. Let's have a look at the distribution of the sensitivity and specificity for each class.

##### Issue of unbalanced classes

The isse of unbalanced classes, as explained previously can highly weekend the model ability to generalize to new data. The model, will automatically prefer to predict the most frequent classes. We can see in the boxplots below the distribution of the sensitivity and specificity for the classes. Even though, we already dealt in part with the unbalanced class during the cleaning process, as seen in the graph (((((((((((((((add graph reference))))))))))))))), there's still big differences between the classes.

```{python, echo = FALSE, output = FALSE}
# Generate predictions on the test set
y_pred = model_no_dropout.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true_classes, y_pred_classes)

# Calculate specificity and sensitivity for each class
tp = np.diag(cm)
fp = cm.sum(axis=0) - np.diag(cm)
fn = cm.sum(axis=1) - np.diag(cm)
tn = cm.sum() - (fp + fn + tp)

# Handle potential division by zero
with np.errstate(divide='ignore', invalid='ignore'):
    sensitivity = np.divide(tp, tp + fn)
    specificity = np.divide(tn, tn + fp)
    # Ensure that the data is in the correct format
    sensitivity = np.array(sensitivity)
    specificity = np.array(specificity)
    # Set sensitivities and specificities to 0 where division by zero occurred
    sensitivity[np.isnan(sensitivity)] = 0
    specificity[np.isnan(specificity)] = 0

# Print specificity and sensitivity for each class
unique_labels = np.unique(y_true_classes)
for i, class_label in enumerate(unique_labels):
    print(f"Class {class_label} - Sensitivity: {sensitivity[i]:.4f}, Specificity: {specificity[i]:.4f}")
```

```{python, echo = FALSE, warning = FALSE}
# Create boxplots
data = [sensitivity, specificity]
fig, ax = plt.subplots()
ax.boxplot(data, labels=["Sensitivity", "Specificity"])
ax.set_title("Sensitivity and Specificity for Each Class")
ax.set_ylabel("Proportion")
plt.show()
```

By examining the boxplot representing the distribution of sensitivity and specificity across the classes, we observe clear evidence of class imbalance. Sensitivity and specificity are not consistent across the classes. Specificity, which measures how well the model identifies true negatives, is very high for every class. This indicates that the model is effective at detecting instances that do not belong to a given class. However, sensitivity, which measures the true positive rate and reflects how well the model correctly predicts the make of a car for a specific brand, is not as high. This suggests that the model is not performing well for all classes.

For some classes with more vehicle models, the model tends to predict those classes more frequently, leading to higher accuracy but lower sensitivity for rarer classes. To address this issue, we will use class weights to ensure the model performs more evenly across all classes.

##### Adding class weights to the model

This technique has been explained in the methods section but in a nutshell, it is a way to penalize the model for misclassifying the minority class more than the majority class. This helps the model to learn the patterns in the minority class better and improve its performance on the test set.

```{python, echo = FALSE, output = FALSE}
# Define the neural network model
model_no_dropout2 = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(y_train.shape[1], activation='softmax')])

# Compile the model
model_no_dropout2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model with class weights
history_no_dropout = model_no_dropout2.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, class_weight=class_weights_dict, verbose=0)

# Evaluate the model
loss_no_dropout, accuracy_no_dropout = model_no_dropout2.evaluate(X_test, y_test)
print(f'Test accuracy without Dropout: {accuracy_no_dropout}')

# Make predictions
predictions_no_dropout = np.argmax(model_no_dropout2.predict(X_test), axis=1)

# Print predictions
print(predictions_no_dropout)
```

```{python, echo = FALSE, output = FALSE}
# Generate predictions on the test set
y_pred = model_no_dropout2.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true_classes, y_pred_classes)

# Calculate specificity and sensitivity for each class
tp = np.diag(cm)
fp = cm.sum(axis=0) - np.diag(cm)
fn = cm.sum(axis=1) - np.diag(cm)
tn = cm.sum() - (fp + fn + tp)

# Handle potential division by zero
with np.errstate(divide='ignore', invalid='ignore'):
    sensitivity = np.divide(tp, tp + fn)
    specificity = np.divide(tn, tn + fp)
    # Set sensitivities and specificities to 0 where division by zero occurred
    sensitivity[np.isnan(sensitivity)] = 0
    specificity[np.isnan(specificity)] = 0

# Print specificity and sensitivity for each class
unique_labels = np.unique(y_true_classes)
for i, class_label in enumerate(unique_labels):
    print(f"Class {class_label} - Sensitivity: {sensitivity[i]:.4f}, Specificity: {specificity[i]:.4f}")
```

```{python, echo = FALSE}
# Create boxplots
data = [sensitivity, specificity]
fig, ax = plt.subplots()
ax.boxplot(data, labels=["Sensitivity", "Specificity"])
ax.set_title("Sensitivity and Specificity for Each Class")
ax.set_ylabel("Proportion")
plt.show()
```

As we can see the model is taking better care of the minority classes and overall the sensitivity is higher across the classes. The sensitivity and specificity are more consistent across the classes. The model is better at generalizing to new data. In our case, this method does not eliminate completely the issue of unbalanced classes. Given the structure of our data and the discrepancy of our classes, we will use this technique for the following neural networks and move on.

##### Model performance

We can now look at the evolution of the accuracy of our model during the training process with the following plot.

```{python, echo = FALSE}
# Plot the accuracy and loss for the model without Dropout
fig, axs = plt.subplots(2, 1, figsize=(10, 10))

# Plot training & validation accuracy values
axs[0].plot(history_no_dropout.history['accuracy'])
axs[0].plot(history_no_dropout.history['val_accuracy'])
axs[0].set_title('Model accuracy without Dropout')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
axs[1].plot(history_no_dropout.history['loss'])
axs[1].plot(history_no_dropout.history['val_loss'])
axs[1].set_title('Model loss without Dropout')
axs[1].set_ylabel('Loss')
axs[1].set_xlabel('Epoch')
axs[1].legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()
```

As we can see, at each epoch, the accuracy is increasing and the loss is decreasing. The model is learning from the training data and improving its predictions.

```{python, echo = FALSE}
# Extract the final training and validation accuracy from the history_no_dropout
final_training_accuracy = history_no_dropout.history['accuracy'][-1]
final_validation_accuracy = history_no_dropout.history['val_accuracy'][-1]
```

But, in the end, we have a case of overfitting. The model performs well on the training data but not as well on the testing data. This is an issue because it limits the possibility of generalizing the model to new data.

**Performance of the model with weighted class:**

```{python, echo = FALSE}
# Print the accuracies with nice formatting
print(f"Final Training Accuracy: {final_training_accuracy * 100:.2f}%")
print(f"Final Validation Accuracy: {final_validation_accuracy * 100:.2f}%")
print(f"Test Set Accuracy: {accuracy_no_dropout * 100:.2f}%")
```

Overall, the performance of the model is still good. However the quality can be improved. To address the issue of overfitting, we will introduce Dropout layers in the neural network.

#### Neural Network with Dropout layers

Dropout layers randomly set a fraction of input units to zero during training, which helps prevent overfitting by forcing the model to learn more robust features. We will tune the dropout rate to find the optimal value that balances training and validation accuracy and that insure to reduce overfitting.

```{python, echo = FALSE}
# Split the data into training and validation sets
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
```

####### We used the following hyperparameters (non-exhaustive list):

* **epochs:** 150 (Corresponds to the number of times the model sees the entire dataset during training.)
* **batch_size:** 32 (Corresponds to the number of samples that the model processes before updating the weights.)
* **validation_split:** 0.2 (Corresponds to the fraction of the training data to be used as validation data.)
* **dropout_rate:** varies (Corresponds to the fraction of neurons to drop during training.)

We will try 5 different dropout rates in addition of the case of no dropout. We will train the model with each dropout rate and evaluate its performance on the validation and test sets. We will then plot the training, validation, and test accuracies for each dropout rate to find the optimal value.

```{python}
#| code-fold: false

# Function to create and compile the model
def create_model(dropout_rate=0.0):
    model = Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(y_train.shape[1], activation='softmax')
    ])
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model
```

```{python, echo = FALSE, output = FALSE}
# Define the dropout rates to test
dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]

# Initialize lists to store accuracies
training_accuracies = []
validation_accuracies = []
test_accuracies = []

# Iterate over the dropout rates
for rate in dropout_rates:
    print(f"Testing model with dropout rate: {rate}")
    model = create_model(dropout_rate=rate)
    
    # Train the model and collect training accuracy
    history = model.fit(X_train_split, y_train_split, epochs=num_epochs, batch_size=32, class_weight=class_weights_dict, verbose=0)
    
    # Evaluate the model on the validation set
    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
    
    # Evaluate the model on the test set
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    
    # Store the last epoch's training accuracy, validation accuracy, and test accuracy
    training_accuracy = history.history['accuracy'][-1]
    training_accuracies.append(training_accuracy)
    validation_accuracies.append(val_accuracy)
    test_accuracies.append(test_accuracy)
    # Print rounded accuracy values
    print(f"Training accuracy: {round(training_accuracy, 5)}, Validation accuracy: {round(val_accuracy, 2)}, Test accuracy: {round(test_accuracy, 2)}")
```

##### Selection of the best dropout rate

```{python, echo = FALSE}
# Plot the accuracies
plt.figure(figsize=(10, 6))
plt.plot(dropout_rates, training_accuracies, label='Training Accuracy')
plt.plot(dropout_rates, validation_accuracies, label='Validation Accuracy')
plt.plot(dropout_rates, test_accuracies, label='Test Accuracy')
plt.xlabel('Dropout Rate')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Dropout Rate')
plt.legend()
plt.show()
```

We can see that the model with a dropout rate of 0.1 has the best balance between reducing drastically the overfitting problem and keeping a good overall accuracy. This model has a good balance between training and validation accuracy, and it generalizes well to new data. It also eliminate the overfitting issue.
We will use this dropout rate of 0.1 to train the final model with dropout layers.

```{python, echo = FALSE}
# Out best dropout rate used for the following model.
dropout_rate_to_plot = 0.1
```

```{python, echo = FALSE, output = FALSE}
# Train the model with the specified dropout rate
model = create_model(dropout_rate=dropout_rate_to_plot)

# Train the model and collect training accuracy and loss
history = model.fit(X_train_split, y_train_split, epochs=num_epochs, batch_size=32, class_weight=class_weights_dict, validation_data=(X_val, y_val), verbose=0)

# Extract accuracy and loss data
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
```

##### Model performance

```{python, echo = FALSE}
# Plot the accuracy and loss for the model with Dropout
fig, axs = plt.subplots(2, 1, figsize=(10, 10))

# Plot training & validation accuracy values
axs[0].plot(training_accuracy)
axs[0].plot(validation_accuracy)
axs[0].set_title('Model accuracy with Dropout')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
axs[1].plot(training_loss)
axs[1].plot(validation_loss)
axs[1].set_title('Model loss with Dropout')
axs[1].set_ylabel('Loss')
axs[1].set_xlabel('Epoch')
axs[1].legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()
```

We see that the model with dropout layers performs better that the one without it. We reached a better accuracy on the validation set and the model is clearly not overfitting as much. It is interesting to note that the validation accuracy is higher than the training accuracy. This is a good sign that the model is generalizing well to new data.


**Performance of the model with weighted class and dropout layers:**

```{python, echo = FALSE}
# Print the final accuracies
# Extract the final epoch's accuracy for the training and validation sets
final_training_accuracy = history.history['accuracy'][-1]
final_validation_accuracy = history.history['val_accuracy'][-1]

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Final Training Accuracy: {final_training_accuracy * 100:.2f}%")
print(f"Final Validation Accuracy: {final_validation_accuracy * 100:.2f}%")
print(f"Test Set Accuracy: {test_accuracy * 100:.2f}%")
```

Our final accuracy of our model are not as great as we had with our first model but the model we are using is at least better at representing the data and generalizing to new data. We aslo computed the Cohen's Kappa score which is a good indicator of the model's performance. And as we can see, the model performs well.


```{python, echo = FALSE, output = FALSE}
# Make predictions on the validation set
y_val_pred = model.predict(X_val)

# Convert probabilities to class labels for multi-class classification
y_val_pred_class = np.argmax(y_val_pred, axis=1)

# Calculate Cohen's Kappa
kappa_score = round(cohen_kappa_score(np.argmax(y_val, axis=1), y_val_pred_class),4)
```

```{python, echo = FALSE}
print(f"Cohen's Kappa Score:{kappa_score * 100:.2f}%")
```

```{python, echo = FALSE}
#### Variable importance ####

# https://romainlhardy.medium.com/estimating-feature-importance-the-easy-way-2ebe970c600c

# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.metrics import accuracy_score
# 
# # Function to calculate feature importance using permutation method
# def get_feature_importance(model, X_test, y_test, j, n=100):
#     # Baseline accuracy
#     y_pred = model.predict(X_test)
#     s = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
#     
#     total = 0.0
#     for i in range(n):
#         perm = np.random.permutation(range(X_test.shape[0]))
#         X_test_ = X_test.copy()
#         X_test_[:, j] = X_test[perm, j]
#         y_pred_ = model.predict(X_test_)
#         s_ij = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_, axis=1))
#         total += s_ij
#     
#     return s - total / n
# 
# # Calculate feature importances for all features
# f = []
# for j in range(X_test.shape[1]):
#     f_j = get_feature_importance(model, X_test, y_test, j, n=100)
#     f.append(f_j)
# 
# # Plot feature importances
# plt.figure(figsize=(10, 5))
# plt.bar(range(X_test.shape[1]), f, color="r", alpha=0.7)
# plt.xticks(ticks=range(X_test.shape[1]))
# plt.xlabel("Feature")
# plt.ylabel("Importance")
# plt.title("Feature Importances")
# plt.show()
```

