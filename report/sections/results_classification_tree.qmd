---
format:
  html:
    code-fold: true
---

# Classification Tree

In this section we are going to perform a classification tree analysis on the dataset. We start first to load the necessary packages and the dataset. We then have prepared the data by encoding categorical variables and splitted it into training and testing sets. We then pruned the tree with different max_depth values to find the optimal tree depth that balances between training and test accuracy.

### Loading R packages, Python libraries and Data Preparation
```{r Loading R packages CT, echo= FALSE, message = FALSE, output = FALSE}
source(here::here("scripts","setup.R"))
```

```{python Loading python Librairies CT, echo = FALSE, message = FALSE, waring = FALSE, output = FALSE}
import subprocess
subprocess.run(['pip3', 'install', "pyprojroot"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
from pyprojroot.here import here
open(here("scripts/setup.py"))
```


```{python Importing necessayry functions and the data, echo = FALSE}
# !pip3 install pyprojroot ----- TO INSTALL IF MISSING
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Load the dataset
file_path = here("data/data_cleaned_reduced.csv")
data = pd.read_csv(file_path)
```


After importing all the necessary libraries and packages, we first started by loading the dataset and identified make as the target variable. We also encoded categorical variables using **Label Encoding** to convert them into numerical values.

```{python Preparing the data, echo = FALSE}
# Preprocess the data
# Assuming 'make' is the target variable and all other columns are features
target = 'make'
features = data.columns.drop(target)

# Encode categorical variables
label_encoders = {}
for column in data.columns:
    if data[column].dtype == 'object':
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le
```

We then splited the dataset into training (80%) and testing (20%). This step is essential for preventing overfitting, try to select the best model configuration by training the model, then assess the final model performance on our "unseen data" of the testing set. This helps to build a robust model that generalizes well to new data. Unfortunately, we will see later that our model is overfitting, meaning that it performs poorly when seeing new data. We will address this issue later. You will find here an plot showing the actual data splitting effectuated. The x axis represent the number of observations in our dataset and the y axis the number of makes. 

```{python Splitting the data, echo = FALSE}
# Split the data into training and testing sets
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

```{python, echo = FALSE}
# !pip install matplotlib seaborn ------- INSTALL IF NEEDED
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the dataset
file_path = here("data/data_cleaned.csv")
data = pd.read_csv(file_path)

# Encode categorical variables
label_encoders = {}
for column in data.columns:
    if data[column].dtype == 'object':
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le

# Split the data into features and target
target = 'make'
features = data.columns.drop(target)
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Combine the train and test data for visualization
train_data = X_train.copy()
train_data[target] = y_train
train_data['Set'] = 'Train'

test_data = X_test.copy()
test_data[target] = y_test
test_data['Set'] = 'Test'

combined_data = pd.concat([train_data, test_data])

# Visualize the data split
plt.figure(figsize=(10, 6))
sns.scatterplot(data=combined_data, x=combined_data.index, y=target, hue='Set', palette=['blue', 'orange'])
plt.title('Visualization of Training and Testing Data Split')
plt.xlabel('Data Index')
plt.ylabel('Target Variable (make)')
plt.legend(title='Dataset')
plt.show()

```

### Training the Decision Tree without Pruning - Results

Starting to train our Decision Tree, we have first decided to do it without any constraints and see the results of it. 

```{python, echo = FALSE}
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, cohen_kappa_score
from sklearn.preprocessing import LabelEncoder
from pyprojroot.here import here
# Train the Decision Tree without specifying max_depth
# Train the Decision Tree without specifying max_depth
clf_no_prune = DecisionTreeClassifier(random_state=42)
clf_no_prune.fit(X_train, y_train)

# Predict and calculate accuracies
y_pred_train_no_prune = clf_no_prune.predict(X_train)
y_pred_test_no_prune = clf_no_prune.predict(X_test)
accuracy_train_no_prune = accuracy_score(y_train, y_pred_train_no_prune)
accuracy_test_no_prune = accuracy_score(y_test, y_pred_test_no_prune)
kappa_train_no_prune = cohen_kappa_score(y_train, y_pred_train_no_prune)
kappa_test_no_prune = cohen_kappa_score(y_test, y_pred_test_no_prune)

print(f"Training Accuracy (without max_depth): {accuracy_train_no_prune:.4f}")
print(f"Test Accuracy (without max_depth): {accuracy_test_no_prune:.4f}")
print(f"Training Cohen's Kappa (without max_depth): {kappa_train_no_prune:.4f}")
print(f"Test Cohen's Kappa (without max_depth): {kappa_test_no_prune:.4f}")

# Visualize the Decision Tree without pruning
# plt.figure(figsize=(20, 10))
# plot_tree(clf_no_prune, filled=True, feature_names=features, class_names=label_encoders[target].classes_)
# plt.title("Decision Tree (without max_depth)")
# plt.show()
```

As we can see, the tree resulting from our training is much complex, takes a lot of time to train and is not interpretable at all. Also, we can see that the accuracy of the training set is higher than both the observed accuracy and cohen's kappa of the test set, attesting that our model is overfitting. 

### Training the Decision Tree with K-fold cross-validation - Results

In

```{python}

# !pip install matplotlib seaborn scikit-learn ------- INSTALL IF NEEDED
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Encode categorical variables
label_encoders = {}
for column in data.columns:
    if data[column].dtype == 'object':
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le

# Split the data into features and target
target = 'make'
features = data.columns.drop(target)
X = data[features]
y = data[target]

# Initialize the classifier
clf = DecisionTreeClassifier(random_state=42)

# Initialize KFold cross-validator
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores: ", cv_scores)
print("Mean cross-validation score: ", cv_scores.mean())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the classifier on the training set
clf.fit(X_train, y_train)

# Predict on the training set and test set
y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

# Calculate the training and test set accuracies
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Training set accuracy: {train_accuracy:.4f}")
print(f"Test set accuracy: {test_accuracy:.4f}")

# Visualize the decision tree
from sklearn.tree import plot_tree

# plt.figure(figsize=(20, 10))
# plot_tree(clf, filled=True, feature_names=features, class_names=label_encoders[target].classes_)
# plt.title('Decision Tree')
# plt.show()
```


### Training the Decision Tree with Pruning method - Results

In order to have a less complex tree and to fight overfittingness, we have decided to prune our tree and then train it. We chose to prune the tree by trying a few max_depth parameter values to control the tree's growth (5, 10, 15, 20, 25, 30). We want here to find the optimal tree depth that balances between training and test accuracy. for visibility reasons, we have decided to represent a classification tree with only 5 max_depth. "None" is actually the training without any max_depth parameter. Here are our new results: 

```{python, echo = FALSE}
# Define a function to train and evaluate the Decision Tree with different max_depth values
def train_and_evaluate_tree(max_depth):
    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
    clf.fit(X_train, y_train)
    y_pred_train = clf.predict(X_train)
    y_pred_test = clf.predict(X_test)
    accuracy_train = accuracy_score(y_train, y_pred_train)
    accuracy_test = accuracy_score(y_test, y_pred_test)
    kappa_train = cohen_kappa_score(y_train, y_pred_train)
    kappa_test = cohen_kappa_score(y_test, y_pred_test)
    return clf, accuracy_train, accuracy_test, kappa_train, kappa_test

# Evaluate the model with different max_depth values
max_depth_values = [5, 10, 15, 20, 25, 30]
results = {}
clfs = {}

for depth in max_depth_values:
    clf, accuracy_train, accuracy_test, kappa_train, kappa_test = train_and_evaluate_tree(depth)
    results[depth] = {
        "Train Accuracy": accuracy_train,
        "Test Accuracy": accuracy_test,
        "Train Cohen's Kappa": kappa_train,
        "Test Cohen's Kappa": kappa_test
    }
    clfs[depth] = clf  # Store the classifier for visualization

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame.from_dict(results, orient='index')

# Print the DataFrame
print(results_df)

# Extract test accuracies to find the best depth
test_scores = results_df["Test Accuracy"].tolist()

# Extract feature importances
# Extract feature importances from the best tree (based on test accuracy)
best_depth = max_depth_values[test_scores.index(max(test_scores))]
best_clf = clfs[best_depth]

# Assuming 'features' is a list of feature names
feature_importances = best_clf.feature_importances_

# Create a DataFrame for easy visualization
importances_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by feature importance in descending order
importances_df = importances_df.sort_values(by='Importance', ascending=False)

# Visualize the Decision Tree for specific max_depth values (e.g., 10, 20)
# depths_to_visualize = [10, 20]
# for depth in depths_to_visualize:
#     plt.figure(figsize=(20, 10))
#     plot_tree(clfs[depth], filled=True, feature_names=features, class_names=label_encoders[target].classes_)
#     plt.title(f"Decision Tree (max_depth={depth})")
#     plt.show()
```

```{python, echo = FALSE}
# Define a function to train and evaluate the Decision Tree with different max_depth values
# def train_and_evaluate_tree(max_depth):
#     clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
#     clf.fit(X_train, y_train)
#     y_pred_train = clf.predict(X_train)
#     y_pred_test = clf.predict(X_test)
#     accuracy_train = accuracy_score(y_train, y_pred_train)
#     accuracy_test = accuracy_score(y_test, y_pred_test)
#     return accuracy_train, accuracy_test
# 
# # Evaluate the model with different max_depth values
# max_depth_values = [5, 10, 15, 20, 25, 30, None]
# results = {}
# 
# for depth in max_depth_values:
#     accuracy_train, accuracy_test = train_and_evaluate_tree(depth)
#     results[depth] = (accuracy_train, accuracy_test)
# 
# # Print the results
# print("max_depth\tTraining Accuracy\tTest Accuracy")
# for depth, (train_acc, test_acc) in results.items():
#     print(f"{depth}\t\t{train_acc:.4f}\t\t{test_acc:.4f}")
```

The accuracy of the model improved as the depth of the tree increased, with a max_depth of 25 or 30 providing the best test accuracy and Cohen's kappa reaching up to 70%. A max_depth of 5 resulted in significantly lower accuracy, indicating that it is not the optimal choice. A max_depth of 10 or 15 seems to be the best compromise between overall accuracy and avoiding overfitting.

Pruning the tree with a max_depth of 25 increased the accuracy from 69.84% to 70%, reducing the gap between the test and training sets. This pruning helps to improve the generalisation performance of the model by preventing it from becoming too complex and overfitting the training data.

### Variable Importance 

As we have used a classification tree for the prediction, the most important variable in the model are the ones at the top of the graph. Let's visualize the ranking of the features importance. 

```{python}
# Visualize the feature importances
plt.figure(figsize=(10, 6))
plt.barh(importances_df['Feature'], importances_df['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Pruned Decision Tree')
plt.gca().invert_yaxis()  # To display the most important feature at the top
plt.show()

# Print the sorted feature importances
# print(importances_df)
```

We can see that among the top 3 most important features figure in order : the engine displacement, the model of the year and the class of the vehicle. On another hand, we can see that some features such as range_ev_city_fuel_type_2, range_ev_highway_fuel_type_2 and range_ev_city_fuel_type_1 are not important for our model for making predictions. This is certainly due to the quantity of EV and hybrid cars in the dataset and these features that are only concerning these types of vehicles.
