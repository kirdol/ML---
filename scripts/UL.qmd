---
title: "Unsupervised Learning"
format: pdf
editor: visual
---

```{r}
library(data.table)
data <- fread(here::here("data", "Vehicle MPG - 1984 to 2023.csv"))
```

```{r}
library(FactoMineR)
data.pca <- PCA(data[, -1, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 21, 22, 23], graph = FALSE)
```

```{r}
head(data)
summary(data)

# Assuming 'make' is a factor (categorical variable) and located in column 3
data[,3] <- as.factor(data[,3])

# Scale numerical features, assuming 'make' is the only non-numerical column
numerical_indices <- setdiff(1:ncol(data), 3)  # Exclude column 3
data[DT[, ..numerical_indices]] <- scaledata[DT[, ..numerical_indices]

# Hierarchical clustering
# Calculate distances (Manhattan) among the scaled numerical features
library(reshape2)  # For melting the distance matrix
library(ggplot2)   # For plotting
data_d <- dist(data[,numerical_indices], method = "manhattan")
data_melt <- melt(as.matrix(data_d))

# Visualizing distances using tiles
ggplot(data = data_melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

# Performing hierarchical clustering using complete linkage
data_hc <- hclust(data_d, method = "complete")
plot(data_hc, hang=-1)

# Cutting the tree to form clusters, and extracting cluster assignments
rect.hclust(data_hc, k=4)  # Modify 'k' as needed based on your analysis
data_clust <- cutree(data_hc, k=4)

# Analysis of clusters
data_comp <- data.frame(data[,numerical_indices], Clust=factor(data_clust), Id=row.names(data))
data_df <- melt(data_comp, id=c("Id", "Clust"))

# Boxplots to analyze cluster distributions
ggplot(data_df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, ncol=4, nrow=3)
```




